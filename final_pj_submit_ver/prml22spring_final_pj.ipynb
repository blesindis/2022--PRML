{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fudan PRML22 Spring Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your name and Student ID: 陈朦伊, 19307110382*\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet, and a .pdf report file) with your assignment submission.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations, you have come to the last challenge!**\n",
    "\n",
    "Having finished the past two assignments, we think all you gugs already have a solid foundation in the field of machine learning and deep learning. And now you are qualified to apply machine learning algorithms to the real-world tasks you are interested in, or start your machine learning research. \n",
    "\n",
    "**In this final project, you are free to choose a topic you are passionate about. The project can be an application one, a theoretical one or implementing your own amazing machine learning/deep learning framework like a toy pytorch. If you don't have any idea, we will also provide you with a default one you can play with.** \n",
    "\n",
    "**! Notice: If you want to work on your own idea, you have to email the TA (lip21[at]m.fudan.edu.cn) to give a simple project proposal first before May 22, 2022.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Project: Natural Language Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sherlock](./img/inference.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default final project this semester is a NLP task called \"Natural Language Inference\". Though deep neural networks have demonstrated astonishing performance in many tasks like text classification and generation, you might somehow think they are just \"advanced statistics\" but far from *intelligent* machines. One intelligent machine must be able to reason, you may think. And in this default final project, your aim is to design a machine which can conduct inference. The machine can know that \"A man inspects the uniform of a figure in some East Asian country\" is contradictory to \"The man is sleeping\", and \"a soccer game with multiple males playing.\" entails \"some men are playing a sport\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we use this time is the Original Chinese Natural Language Inference (OCNLI) dataset[1]. It is a chinese NLI dataset with about 50k training data and 3k development data. The sentence pairs in the dataset are labeled as \"entailment\", \"neutral\" and \"contradiction\". Due to they release the test data without its labels, we select 5k data pairs from the training data as labeled test data, and the other 45k data as your t. You can visit the [GitHub link](https://github.com/CLUEbenchmark/OCNLI) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you finished the NLI task with the full 50k training set, you have to complete an advanced challenge. You have to select **at most 5k data** from the training set as labeled training set, leaving the other training data as unlabeled training set, then use these labeled and unlabeled data to finish the same NLI task. You can randomly choosing the 5k training data but can also think up some ideas to select more **important data** as labeled training data. Like assignment1, you may have to think how to use the unlabeled training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the deep learning frameworks like paddle, pytorch, tensorflow in your experiment but not more high-level libraries like Huggingface. Please write down the version of them in the './requirements.txt' file.\n",
    "\n",
    "**! Notice: You CAN NOT use any other people's pretrained model like 'bert-base-chinese' in this default project. You are encouraged to design your own model and algorithm, no matter it looks naive or not.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLI is a traditional but promising NLP task, and you can search the Google/Bing for more information. Some key words can be \"natural language inference with attention\", \"training data selection\", \"semi-surpervised learning\", \"unsupervised representation learning\" and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "import the libraries and load the dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# setup code\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import Vectors\n",
    "from model.util import load_iters\n",
    "from model.models import ESIM\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './dataset'\n",
    "\n",
    "train_data_file = dataset_path + '/train.json'\n",
    "dev_data_file = dataset_path + '/dev.json'\n",
    "test_data_file = dataset_path + '/test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from  ./dataset/train.json\n",
      "there are  45437 sentence pairs in this file.\n",
      "loading data from  ./dataset/dev.json\n",
      "there are  2950 sentence pairs in this file.\n",
      "loading data from  ./dataset/test.json\n",
      "there are  5000 sentence pairs in this file.\n"
     ]
    }
   ],
   "source": [
    "def read_ocnli_file(data_file):\n",
    "    # read the ocnli file. feel free to change it. \n",
    "    print (\"loading data from \", data_file)\n",
    "    \n",
    "    text_outputs = []\n",
    "    label_outputs = []\n",
    "    \n",
    "    label_to_idx = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "    \n",
    "    with open(data_file, 'r', encoding=\"utf-8\") as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            line = json.loads(line.strip())\n",
    "            text_a, text_b, label = line['sentence1'], line['sentence2'],line['label']\n",
    "            label_id = label_to_idx[label.strip()]\n",
    "            \n",
    "            text_outputs.append((text_a,text_b))\n",
    "            label_outputs.append(label_id)\n",
    "\n",
    "            line = f.readline()\n",
    "                \n",
    "    print (\"there are \", len(label_outputs), \"sentence pairs in this file.\")\n",
    "    return text_outputs, label_outputs\n",
    "\n",
    "\n",
    "training_data, training_labels = read_ocnli_file(train_data_file)\n",
    "dev_data, dev_labels = read_ocnli_file(dev_data_file)\n",
    "test_data, test_labels = read_ocnli_file(test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data samples:  [('对,对,对,对,对,具体的答复.', '要的是抽象的答复'), ('当前国际形势仍处于复杂而深刻的变动之中', '一个月后将发生世界战争'), ('在全县率先推行宅基地有偿使用,全乡20年无须再扩大宅基地', '宅基地有偿使用获得较好成果,将在更大范围实施。'), ('上海马路上的喧声也是老调子', '上海有很多条马路'), ('那你看看第二封信什么时候到吧.', '第一封信已经收到了。')]\n",
      "training labels samples:  [2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print (\"training data samples: \", training_data[:5])\n",
    "print (\"training labels samples: \", training_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your may have to explore the dataset and do some analysis first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  45437\n",
      "Dev size:  2950\n",
      "Test size:  5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training size: \",len(training_data))\n",
    "print(\"Dev size: \",len(dev_data))\n",
    "print(\"Test size: \", len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and evaluate your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def show_example(premise, hypothesis, label, TEXT, LABEL):\n",
    "    tqdm.write('Label: ' + LABEL.vocab.itos[label])\n",
    "    tqdm.write('premise: ' + ' '.join([TEXT.vocab.itos[i] for i in premise]))\n",
    "    tqdm.write('hypothesis: ' + ' '.join([TEXT.vocab.itos[i] for i in hypothesis]))\n",
    "\n",
    "\n",
    "def eval(data_iter, name, epoch=None, use_cache=False):\n",
    "    if use_cache:\n",
    "        model.load_state_dict(torch.load('best_model.ckpt'))\n",
    "    model.eval()\n",
    "    correct_num = 0\n",
    "    err_num = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            premise, premise_lens = batch.premise\n",
    "            hypothesis, hypothesis_lens = batch.hypothesis\n",
    "            labels = batch.label\n",
    "\n",
    "            output = model(premise, premise_lens, hypothesis, hypothesis_lens)\n",
    "            predicts = output.argmax(-1).reshape(-1)\n",
    "            loss = loss_func(output, labels)\n",
    "            total_loss += loss.item()\n",
    "            correct_num += (predicts == labels).sum().item()\n",
    "            err_num += (predicts != batch.label).sum().item()\n",
    "\n",
    "    acc = correct_num / (correct_num + err_num)\n",
    "    if epoch is not None:\n",
    "        tqdm.write(\n",
    "            \"Epoch: %d, %s Acc: %.3f, Loss %.3f\" % (epoch + 1, name, acc, total_loss))\n",
    "    else:\n",
    "        tqdm.write(\n",
    "            \"%s Acc: %.3f, Loss %.3f\" % (name, acc, total_loss))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train(train_iter, dev_iter, loss_func, optimizer, epochs, patience=5, clip=5):\n",
    "    best_acc = -1\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_iter):\n",
    "            premise, premise_lens = batch.premise\n",
    "            hypothesis, hypothesis_lens = batch.hypothesis\n",
    "            labels = batch.label\n",
    "            # show_example(premise[0],hypothesis[0], labels[0], TEXT, LABEL)\n",
    "\n",
    "            model.zero_grad()\n",
    "            output = model(premise, premise_lens, hypothesis, hypothesis_lens)\n",
    "            loss = loss_func(output, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "        tqdm.write(\"Epoch: %d, Train Loss: %d\" % (epoch + 1, total_loss))\n",
    "\n",
    "        acc = eval(dev_iter, \"Dev\", epoch)\n",
    "        if acc<best_acc:\n",
    "            patience_counter +=1\n",
    "        else:\n",
    "            best_acc = acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.ckpt')\n",
    "        if patience_counter >= patience:\n",
    "            tqdm.write(\"Early stopping: patience limit reached, stopping...\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torchtext/data/example.py:13: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "data_path = 'dataset'\n",
    "vectors = None\n",
    "freeze = False\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 600  # every LSTM's(forward and backward) hidden size is half of HIDDEN_SIZE\n",
    "EPOCHS = 20\n",
    "DROPOUT_RATE = 0.5\n",
    "LAYER_NUM = 1\n",
    "LEARNING_RATE = 4e-4\n",
    "PATIENCE = 5\n",
    "CLIP = 10\n",
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "train_iter, dev_iter, test_iter, TEXT, LABEL, _ = load_iters(BATCH_SIZE, device, data_path, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,561,203 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1420/1420 [00:36<00:00, 39.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1560\n",
      "Epoch: 1, Dev Acc: 0.374, Loss 101.965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1420/1420 [00:35<00:00, 39.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 1560\n",
      "Epoch: 2, Dev Acc: 0.305, Loss 103.415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1420/1420 [00:36<00:00, 39.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 1415\n",
      "Epoch: 3, Dev Acc: 0.375, Loss 105.086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1420/1420 [00:36<00:00, 39.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 797\n",
      "Epoch: 4, Dev Acc: 0.322, Loss 128.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1420/1420 [00:35<00:00, 39.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 256\n",
      "Epoch: 5, Dev Acc: 0.323, Loss 267.510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1420/1420 [00:35<00:00, 39.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 81\n",
      "Epoch: 6, Dev Acc: 0.323, Loss 423.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1420/1420 [00:36<00:00, 38.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 37\n",
      "Epoch: 7, Dev Acc: 0.323, Loss 483.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1420/1420 [00:36<00:00, 39.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 27\n",
      "Epoch: 8, Dev Acc: 0.323, Loss 477.412\n",
      "Early stopping: patience limit reached, stopping...\n"
     ]
    }
   ],
   "source": [
    "model = ESIM(len(TEXT.vocab), len(LABEL.vocab.stoi),\n",
    "                 EMBEDDING_SIZE, HIDDEN_SIZE, DROPOUT_RATE, LAYER_NUM,\n",
    "                 TEXT.vocab.vectors, freeze).to(device)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "train(train_iter, dev_iter, loss_func, optimizer, EPOCHS,PATIENCE, CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.340, Loss 180.162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(test_iter, \"Test\", use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attention Visualization (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the attention matrix in your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Attack (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attack your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down your conclusion here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次pj让我们大致了解了NLI任务的工作流程与基本实现方法，然而由于期末季时间太紧，没能实现第二个挑战，十分抱歉。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] OCNLI: Original Chinese Natural Language Inference, arxiv: https://arxiv.org/abs/2010.05444"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccc93251d6f721491c6fbc0c6bcbe4e9a605d28f77b9a8e19b15e1c61d18557e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('myconda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
