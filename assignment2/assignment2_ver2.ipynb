{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fudan PRML Assignment2: Machine Translation and Model Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Google Translation](./img/google_translation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your name and Student ID: 陈朦伊 19307110382*\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet, and a .pdf report file) with your assignment submission.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你好，欢迎来到第二次作业！    \n",
    "Hello and welcome to the second assignment!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will work on a **Chinese to English** machine translation (MT) task with neural networks. Different from assignment1, this is a generation task in the field of NLP. The bilingual parallel corpus you will play with is the News Commentary v13 dataset from the [Third Conference on Machine Learning (WMT18)](https://www.statmt.org/wmt18/translation-task.html). There are about 252700 training samples, 2000 validation samples and 2000 test samples in the dataset. And the Chinese sentences have been processed by word segmentation. You have to design a Sequence to Sequence (Seq2Seq) model to complete this translation task. **The Attention mechanism must be used in your model** but you are free to design other modules with CNNs, RNNs and so on. **You have to evaluate your model on the test set with [Bilingual Evaluation Understudy (BELU) score](https://en.wikipedia.org/wiki/BLEU) and [Perplexity](https://en.wikipedia.org/wiki/Perplexity)**. **Besides, you have to visualize the attention matrix to help you understand your model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you building your model, in the second part, you have to **attack** it : ) AI safety is nowadays a popular research point and many kind of attack methods have been developed during the the past few years. These methods include adversarial attack, data poisoning attack, training data leakage attack and so on [1]. In this assignment, **you just need to conduct one type of attack**. You can choose a simplest one or just analyze what kind of samples the model will predict incorrectly. The important thing is to understand the behavior of the neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the deep learning frameworks like paddle, pytorch, tensorflow in your experiment but not more high-level libraries like Huggingface. Please write down the version of them in the './requirements.txt' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following links may be useful:    \n",
    "- *Machine Translation: Foundations and Models,Tong Xiao and Jingbo Zhu, link: https://github.com/NiuTrans/MTBook*\n",
    "- PyTorch Seq2Seq Tutorial @ Ben Trevett, link: https://github.com/bentrevett/pytorch-seq2seq\n",
    "\n",
    "Certainly, our exercises in the PaddlePaddle AI Studio Platform will be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "import the libraries and load the dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc72041acf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Processing datasets...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "dataset_path = './dataset'\n",
    "train_en_path = os.path.join(dataset_path, 'train', 'news-commentary-v13.zh-en.en')\n",
    "train_zh_path = os.path.join(dataset_path, 'train', 'news-commentary-v13.zh-en.zh')\n",
    "dev_en_path = os.path.join(dataset_path, 'dev', 'newsdev2017.tc.en')\n",
    "dev_zh_path = os.path.join(dataset_path, 'dev', 'newsdev2017.tc.zh')\n",
    "test_en_path = os.path.join(dataset_path, 'test', 'newstest2017.tc.en')\n",
    "test_zh_path = os.path.join(dataset_path, 'test', 'newstest2017.tc.zh')\n",
    "\n",
    "paths = {'train_en': train_en_path, 'train_zh': train_zh_path, 'dev_en': dev_en_path, 'dev_zh': dev_zh_path, 'test_en': test_en_path, 'test_zh': test_zh_path}\n",
    "data_bundle = data_loader.DataBundle(paths)\n",
    "data_bundle.process()\n",
    "print(\"Processing datasets...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your may have to explore the dataset and do some analysis first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n",
      "\tTrain: 252777\n",
      "\tDev: 2002\n",
      "\tTest: 2001\n",
      "Vocabulary:\n",
      "\tEn: 166192\n",
      "\tZh: 93264\n"
     ]
    }
   ],
   "source": [
    "data_bundle.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and evaluate your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining en embedding...\n",
      "Pretraining zh embedding...\n"
     ]
    }
   ],
   "source": [
    "#en_embed = embedding.Embedding(vocab=data_bundle.get_vocab('en'), model_name='glove.6B.300d.txt', word_type='en')\n",
    "#zh_embed = embedding.Embedding(vocab=data_bundle.get_vocab('zh'), model_name='cc.zh.300.vec', word_type='zh')\n",
    "en_embed = embedding.Embedding(vocab=data_bundle.get_vocab('en'), model_name=None, word_type='en')\n",
    "zh_embed = embedding.Embedding(vocab=data_bundle.get_vocab('zh'), model_name=None, word_type='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab = data_bundle.get_vocab('zh')\n",
    "output_vocab = data_bundle.get_vocab('en')\n",
    "input_var = data_bundle.get_dataset('train')['zh']\n",
    "output_var = data_bundle.get_dataset('train')['en']\n",
    "pairs = list(zip(input_var, output_var))\n",
    "pairs.sort(key=lambda x:len(x[0].split()), reverse=True)\n",
    "input_var, output_var = zip(*pairs)\n",
    "#strip empty input\n",
    "input_var = input_var[:1000]\n",
    "output_var = output_var[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embedding): Embedding(\n",
       "    (embedding): Embedding(166192, 300, padding_idx=0)\n",
       "    (dropout_layer): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (gru): GRU(300, 300, num_layers=2, dropout=0.1)\n",
       "  (concat): Linear(in_features=600, out_features=300, bias=True)\n",
       "  (out): Linear(in_features=300, out_features=166192, bias=True)\n",
       "  (attn): Attn()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batches = utils.batch2TrainData(input_vocab, output_vocab, input_var, output_var)\n",
    "#input_var, lengths, output_var, mask, max_target_len = batches\n",
    "\n",
    "model_name = 'nmt'\n",
    "attn_model = 'dot'\n",
    "batch_size = 50\n",
    "hidden_size = 300\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "clip = 50.0\n",
    "n_iteration = 100\n",
    "\n",
    "\n",
    "encoder = models.EncoderRNN(hidden_size, zh_embed, encoder_n_layers, dropout)\n",
    "decoder = models.DecoderRNN('dot', en_embed, hidden_size, len(output_vocab), decoder_n_layers, dropout)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "#encoder_outputs, encoder_hidden = encoder(input_var, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Start training...\n",
      "torch.Size([87, 50, 300])\n",
      "torch.Size([109, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([92, 50, 300])\n",
      "torch.Size([108, 50, 300])\n",
      "torch.Size([108, 50, 300])\n",
      "torch.Size([129, 50, 300])\n",
      "torch.Size([115, 50, 300])\n",
      "torch.Size([94, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "Iteration: 10; Percent complete: 10.0%; Average loss: 11.7894\n",
      "torch.Size([109, 50, 300])\n",
      "torch.Size([95, 50, 300])\n",
      "torch.Size([101, 50, 300])\n",
      "torch.Size([97, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([109, 50, 300])\n",
      "torch.Size([110, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "Iteration: 20; Percent complete: 20.0%; Average loss: 9.7012\n",
      "torch.Size([90, 50, 300])\n",
      "torch.Size([89, 50, 300])\n",
      "torch.Size([111, 50, 300])\n",
      "torch.Size([96, 50, 300])\n",
      "torch.Size([96, 50, 300])\n",
      "torch.Size([88, 50, 300])\n",
      "torch.Size([89, 50, 300])\n",
      "torch.Size([94, 50, 300])\n",
      "torch.Size([102, 50, 300])\n",
      "torch.Size([111, 50, 300])\n",
      "Iteration: 30; Percent complete: 30.0%; Average loss: 8.3140\n",
      "torch.Size([108, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([92, 50, 300])\n",
      "torch.Size([94, 50, 300])\n",
      "torch.Size([101, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([90, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([101, 50, 300])\n",
      "torch.Size([110, 50, 300])\n",
      "Iteration: 40; Percent complete: 40.0%; Average loss: 7.9710\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([110, 50, 300])\n",
      "torch.Size([104, 50, 300])\n",
      "torch.Size([89, 50, 300])\n",
      "torch.Size([96, 50, 300])\n",
      "torch.Size([92, 50, 300])\n",
      "torch.Size([115, 50, 300])\n",
      "torch.Size([96, 50, 300])\n",
      "torch.Size([111, 50, 300])\n",
      "torch.Size([90, 50, 300])\n",
      "Iteration: 50; Percent complete: 50.0%; Average loss: 7.8922\n",
      "torch.Size([109, 50, 300])\n",
      "torch.Size([95, 50, 300])\n",
      "torch.Size([129, 50, 300])\n",
      "torch.Size([104, 50, 300])\n",
      "torch.Size([129, 50, 300])\n",
      "torch.Size([90, 50, 300])\n",
      "torch.Size([101, 50, 300])\n",
      "torch.Size([105, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([105, 50, 300])\n",
      "Iteration: 60; Percent complete: 60.0%; Average loss: 7.8868\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([101, 50, 300])\n",
      "torch.Size([100, 50, 300])\n",
      "torch.Size([101, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([110, 50, 300])\n",
      "torch.Size([129, 50, 300])\n",
      "torch.Size([89, 50, 300])\n",
      "torch.Size([99, 50, 300])\n",
      "torch.Size([101, 50, 300])\n",
      "Iteration: 70; Percent complete: 70.0%; Average loss: 7.8780\n",
      "torch.Size([97, 50, 300])\n",
      "torch.Size([105, 50, 300])\n",
      "torch.Size([86, 50, 300])\n",
      "torch.Size([104, 50, 300])\n",
      "torch.Size([111, 50, 300])\n",
      "torch.Size([110, 50, 300])\n",
      "torch.Size([115, 50, 300])\n",
      "torch.Size([101, 50, 300])\n",
      "torch.Size([102, 50, 300])\n",
      "torch.Size([101, 50, 300])\n",
      "Iteration: 80; Percent complete: 80.0%; Average loss: 7.8169\n",
      "torch.Size([97, 50, 300])\n",
      "torch.Size([89, 50, 300])\n",
      "torch.Size([115, 50, 300])\n",
      "torch.Size([129, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([115, 50, 300])\n",
      "torch.Size([100, 50, 300])\n",
      "torch.Size([108, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "Iteration: 90; Percent complete: 90.0%; Average loss: 7.7890\n",
      "torch.Size([90, 50, 300])\n",
      "torch.Size([111, 50, 300])\n",
      "torch.Size([98, 50, 300])\n",
      "torch.Size([94, 50, 300])\n",
      "torch.Size([115, 50, 300])\n",
      "torch.Size([105, 50, 300])\n",
      "torch.Size([96, 50, 300])\n",
      "torch.Size([91, 50, 300])\n",
      "torch.Size([90, 50, 300])\n",
      "torch.Size([100, 50, 300])\n",
      "Iteration: 100; Percent complete: 100.0%; Average loss: 7.7825\n"
     ]
    }
   ],
   "source": [
    "trainer.trainIters(model_name, input_vocab, output_vocab, input_var, output_var, encoder, decoder,\n",
    "                   encoder_optimizer, decoder_optimizer, en_embed, zh_embed, encoder_n_layers, decoder_n_layers, \n",
    "                   n_iteration, batch_size, clip, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1, 300])\n",
      "the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "searcher = trainer.Searcher(encoder, decoder)\n",
    "words = trainer.evaluate(encoder, decoder, searcher, input_vocab, output_vocab, \"我 喜欢 你 啊 啊 啊 啊\", 0)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attention Visualization (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the attention matrix in your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Attack (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attack your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down your conclusion here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Must-read Papers on Textual Adversarial Attack and Defense, GitHub: https://github.com/thunlp/TAADpapers"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c168a27b8941e2a4dbca4673a7535dd8b3e5a3e044d109aa43ed058cc3fe3a00"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
