{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fudan PRML Assignment2: Machine Translation and Model Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Google Translation](./img/google_translation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your name and Student ID: 陈朦伊 19307110382*\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet, and a .pdf report file) with your assignment submission.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你好，欢迎来到第二次作业！    \n",
    "Hello and welcome to the second assignment!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will work on a **Chinese to English** machine translation (MT) task with neural networks. Different from assignment1, this is a generation task in the field of NLP. The bilingual parallel corpus you will play with is the News Commentary v13 dataset from the [Third Conference on Machine Learning (WMT18)](https://www.statmt.org/wmt18/translation-task.html). There are about 252700 training samples, 2000 validation samples and 2000 test samples in the dataset. And the Chinese sentences have been processed by word segmentation. You have to design a Sequence to Sequence (Seq2Seq) model to complete this translation task. **The Attention mechanism must be used in your model** but you are free to design other modules with CNNs, RNNs and so on. **You have to evaluate your model on the test set with [Bilingual Evaluation Understudy (BELU) score](https://en.wikipedia.org/wiki/BLEU) and [Perplexity](https://en.wikipedia.org/wiki/Perplexity)**. **Besides, you have to visualize the attention matrix to help you understand your model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you building your model, in the second part, you have to **attack** it : ) AI safety is nowadays a popular research point and many kind of attack methods have been developed during the the past few years. These methods include adversarial attack, data poisoning attack, training data leakage attack and so on [1]. In this assignment, **you just need to conduct one type of attack**. You can choose a simplest one or just analyze what kind of samples the model will predict incorrectly. The important thing is to understand the behavior of the neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the deep learning frameworks like paddle, pytorch, tensorflow in your experiment but not more high-level libraries like Huggingface. Please write down the version of them in the './requirements.txt' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following links may be useful:    \n",
    "- *Machine Translation: Foundations and Models,Tong Xiao and Jingbo Zhu, link: https://github.com/NiuTrans/MTBook*\n",
    "- PyTorch Seq2Seq Tutorial @ Ben Trevett, link: https://github.com/bentrevett/pytorch-seq2seq\n",
    "\n",
    "Certainly, our exercises in the PaddlePaddle AI Studio Platform will be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "import the libraries and load the dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field, BucketIterator, Example\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import model.DataLoader as data_loader\n",
    "import model.utils as utils\n",
    "import model.Trainer2 as trainer\n",
    "from model.Datasets import MyDataset\n",
    "from model.models import Encoder, Decoder, Seq2Seq\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './dataset'\n",
    "train_en_path = os.path.join(dataset_path, 'train', 'news-commentary-v13.zh-en.en')\n",
    "train_zh_path = os.path.join(dataset_path, 'train', 'news-commentary-v13.zh-en.zh')\n",
    "dev_en_path = os.path.join(dataset_path, 'dev', 'newsdev2017.tc.en')\n",
    "dev_zh_path = os.path.join(dataset_path, 'dev', 'newsdev2017.tc.zh')\n",
    "test_en_path = os.path.join(dataset_path, 'test', 'newstest2017.tc.en')\n",
    "test_zh_path = os.path.join(dataset_path, 'test', 'newstest2017.tc.zh')\n",
    "\n",
    "paths = {'train_en': train_en_path, 'train_zh': train_zh_path, 'dev_en': dev_en_path, 'dev_zh': dev_zh_path, 'test_en': test_en_path, 'test_zh': test_zh_path}\n",
    "data_bundle = data_loader.DataBundle(paths)\n",
    "data_bundle.process()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your may have to explore the dataset and do some analysis first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n",
      "\tTrain: 252777\n",
      "\tDev: 2002\n",
      "\tTest: 2001\n",
      "Vocabulary:\n",
      "\tEn: 166192\n",
      "\tZh: 93264\n"
     ]
    }
   ],
   "source": [
    "data_bundle.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and evaluate your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "570\n",
      "686\n",
      "700\n",
      "1251\n",
      "1464\n",
      "2773\n",
      "3249\n",
      "3535\n",
      "3746\n",
      "4586\n",
      "4982\n",
      "4987\n"
     ]
    }
   ],
   "source": [
    "SRC = Field(tokenize = utils.tokenize, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = utils.tokenize, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "train_data = MyDataset(data_bundle.get_dataset('train')['zh'][:1000], data_bundle.get_dataset('train')['en'][:1000], (SRC, TRG))\n",
    "valid_data = MyDataset(data_bundle.get_dataset('dev')['zh'], data_bundle.get_dataset('dev')['en'], (SRC, TRG))\n",
    "test_data = MyDataset(data_bundle.get_dataset('test')['zh'], data_bundle.get_dataset('test')['en'], (SRC, TRG))\n",
    "\n",
    "trg_data, src_data = data_bundle.get_dataset('train')['en'][:5000], data_bundle.get_dataset('train')['zh'][:5000]\n",
    "\n",
    "#find 朋友 in train\n",
    "for i, data in enumerate(src_data):\n",
    "    words = data.split()\n",
    "    for word in words:\n",
    "        if word.find('朋友') != -1:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attention Visualization (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the attention matrix in your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Attack (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attack your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down your conclusion here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Must-read Papers on Textual Adversarial Attack and Defense, GitHub: https://github.com/thunlp/TAADpapers"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c168a27b8941e2a4dbca4673a7535dd8b3e5a3e044d109aa43ed058cc3fe3a00"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
